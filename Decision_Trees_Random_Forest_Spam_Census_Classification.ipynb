{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meghana Ravikumar\n",
    "# Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import scipy.io\n",
    "import math\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following class is used to import and preprocess data \n",
    "### The census data is read line by line into a numpy matrix. The missing values are preprocessed by using a kNN approach (implemented partially through sklearn), and the categorical data is preprocessed and mapped into discrete categories. The descrete categories are used to make the data more generalizable and take less space in memory to process. The kNN approach was used to fill the variables to ensure that the missing data was accurately substituted. This allows for a better fitting decision tree and more accurate predictions. \n",
    "### The spam data was processed through sklearn by loadmat. There were no missing values in the spam dataset. In order to improve accuracy of predictions and develop a better fitting tree, the spam data is binarized (through the sklearn.binarize package) and more features were added. Specifically the following features were added to featurize.py:  'monies', 'viagra', 'unsubscribe', 'nigeria', 'supplement', 'monetary', 'fund', 'warn', 'porn', 'staff', 'attach', '%', '-', '.', '?', '^', '@', '\\'\n",
    "### After a manual examination of many of the spam emails, these features were found to dominate several of these emails. In order to increas accuracy in prediction, the process of feature extraction can be automated to mine for the overall top 10 features in all ham and spam email. This mining will allow for a wider variety of words to be selected that span both categories of emails, leading to a less biased decision tree. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class importData(object):\n",
    "    def __init__(self,myPath, categorical_data, continuous_data):\n",
    "        self.columns = None\n",
    "        self.path = myPath\n",
    "        self.data = None\n",
    "        self.missing = None #dictionary\n",
    "        self.filledValues = None\n",
    "        self.transformedData = None\n",
    "        self.categorical_data = categorical_data\n",
    "        self.continuous_data = continuous_data\n",
    "\n",
    "    def readingCensusData(self):\n",
    "        census_matrix = np.array([])\n",
    "        with open(self.path,\"rb\") as f:\n",
    "            spamreader = csv.reader(f, delimiter = \",\")\n",
    "            for row in spamreader:\n",
    "                if len(census_matrix)==0:\n",
    "                    census_matrix = np.array(row).reshape(1,len(row))\n",
    "                else:\n",
    "                    data = np.array(row).reshape(1,len(row))\n",
    "                    census_matrix = np.vstack((census_matrix,data))\n",
    "        self.data = census_matrix\n",
    "        return (census_matrix)\n",
    "\n",
    "    def readingHousingData(self):\n",
    "        spam_dataset = scipy.io.loadmat(self.path)\n",
    "        spam_dataset_labels = spam_dataset['training_labels'][0]\n",
    "        spam_dataset_train = spam_dataset['training_data']\n",
    "        spam_dataset_test = spam_dataset['test_data']\n",
    "        return(spam_dataset_labels,spam_dataset_train,spam_dataset_test)\n",
    "\n",
    "    def transformingData(self):\n",
    "        transformedData = self.data\n",
    "        categorial_data = self.categorical_data\n",
    "        continuous_data = self.continuous_data\n",
    "        missing = np.where(transformedData == \"?\")\n",
    "        missing_values = {}\n",
    "\n",
    "        for i in range(len(missing[0])):\n",
    "            if missing[1][i] not in missing_values.keys():\n",
    "                missing_values[missing[1][i]]=[missing[0][i]]\n",
    "            else:\n",
    "                missing_values[missing[1][i]].append(missing[0][i])\n",
    "\n",
    "        for c in categorial_data:\n",
    "            transformedData[:,c][1:]= preprocessing.LabelEncoder().fit_transform(transformedData[:,c][1:])\n",
    "\n",
    "        for t in continuous_data:\n",
    "            transformedData[:,t][1:] = transformedData[:,t][1:]\n",
    "        self.missing = missing_values\n",
    "        self.transformedData = transformedData\n",
    "        \n",
    "        return (self.transformedData, self.missing)\n",
    "    \n",
    "    def convertingToFloat(self):\n",
    "        '''method takes in a numpy array and converts everything to a float'''\n",
    "        transformedData = self.transformedData\n",
    "        convertedData = np.array([])\n",
    "        for col in range(0,transformedData.shape[1]):\n",
    "            currData = transformedData[:,col][1:].astype(float).reshape(len(transformedData[:,col][1:]),1)\n",
    "            if len(convertedData)==0:\n",
    "                convertedData = currData.astype(float)\n",
    "            else:\n",
    "                convertedData = np.hstack((convertedData,currData))\n",
    "        return convertedData\n",
    "\n",
    "    def prep_kNN(self):\n",
    "        convertedData = self.convertingToFloat()\n",
    "        missing_values = self.missing\n",
    "\n",
    "        for column_value in missing_values.keys():\n",
    "            rowValues = missing_values[column_value]\n",
    "            testMat = np.array([])\n",
    "            trainingData = np.array([])\n",
    "            trainingLabels = np.delete(convertedData[:,column_value],rowValues+[0])\n",
    "\n",
    "            #iterate through matrix and split into test and training set\n",
    "            for ind in range(1,convertedData.shape[0]):\n",
    "                currData = np.hstack((convertedData[ind,0:column_value],\n",
    "                                  convertedData[ind,column_value+1:]))\n",
    "                if ind in rowValues:\n",
    "                    if len(testMat) == 0:\n",
    "                        testMat = currData\n",
    "                    else:\n",
    "                        testMat = np.vstack((testMat, currData))\n",
    "                else:\n",
    "                    if len(trainingData)==0:\n",
    "                        trainingData= currData\n",
    "                    else:\n",
    "                        trainingData = np.vstack((trainingData, currData))\n",
    "                    \n",
    "            predicted = self.kNN(testMat, trainingData, trainingLabels) \n",
    "            #np.array -- indexing should be the same as given rowValues\n",
    "            #need to index transformedData to put in new values\n",
    "            for r in range(len(rowValues)):\n",
    "                convertedData[rowValues[r], column_value] = predicted[r]\n",
    "        self.filledValues = convertedData\n",
    "        return self.filledValues \n",
    "\n",
    "    def kNN(self,testMat, trainingData, trainingLabels): #input is a list of test, training, and label set\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(trainingData, trainingLabels)\n",
    "        predictions = knn.predict(testMat)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splittingData(spam_data_train):\n",
    "    numVal = int(spam_data_train.shape[0]/3)\n",
    "    numTrain = spam_data_train.shape[0] - int(numVal)\n",
    "    valIndices = random.sample(range(0,spam_data_train.shape[0]), numVal)\n",
    "    validation_data = np.zeros((numVal,spam_data_train.shape[1]))\n",
    "    training_data = np.zeros((numTrain,spam_data_train.shape[1]))\n",
    "    valInd = 0\n",
    "    trainInd = 0\n",
    "    for spam_ind in range(spam_data_train.shape[0]):\n",
    "        if spam_ind in valIndices:\n",
    "            validation_data[valInd]= spam_data_train[spam_ind,:]\n",
    "            valInd = valInd +1\n",
    "        else:\n",
    "            training_data[trainInd] = spam_data_train[spam_ind,:]\n",
    "            trainInd = trainInd + 1\n",
    "    return (validation_data,training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importing_censusData():\n",
    "    census_object = importData(\"./census_data/train_data.csv\", [1,3,5,6,7,8,9,13], [0,2,4,10,11,12] )\n",
    "    census_matrix = census_object.readingCensusData()\n",
    "    transformedData, missing_values = census_object.transformingData()\n",
    "    filled_transformed = census_object.prep_kNN()\n",
    "    filled_transformed_permuted = np.random.permutation(filled_transformed[1:])\n",
    "    (census_train, census_validate) = splittingData(filled_transformed_permuted)\n",
    "    return (census_train, census_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importing_censusTest():\n",
    "    census_object2 = importData(\"./census_data/test_data.csv\", [1,3,5,6,7,8,9,13], [0,2,4,10,11,12] )\n",
    "    census_matrix2 = census_object2.readingCensusData()\n",
    "    transformedData2, missing_values2 = census_object2.transformingData()\n",
    "    filled_transformed2 = census_object2.prep_kNN()\n",
    "    return (filled_transformed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_processingSpam():\n",
    "    (spam_dataset_labels,spam_dataset_train,\n",
    "     spam_dataset_test) = importData(\"./spam-dataset/spam_data.mat\",[],[]).readingHousingData()\n",
    "    spam_dataset_labels = spam_dataset_labels.reshape(spam_dataset_labels.shape[0],1)\n",
    "    spam_dataset_train_labels = np.concatenate((spam_dataset_train, spam_dataset_labels),axis=1)\n",
    "    spam_dataset_permutation = np.random.permutation(spam_dataset_train_labels)\n",
    "    spam_data_permutation = preprocessing.binarize(spam_dataset_permutation)\n",
    "    (spam_dataset_train, spam_dataset_validate) = splittingData(spam_dataset_permutation)\n",
    "    return (spam_dataset_train, spam_dataset_validate, spam_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prep knn\n",
      "(32724, 15)\n",
      "in knn\n",
      "in knn\n",
      "in knn\n"
     ]
    }
   ],
   "source": [
    "(census_train, census_validate) = importing_censusData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prep knn\n",
      "(16118, 14)\n",
      "in knn\n",
      "in knn\n",
      "in knn\n"
     ]
    }
   ],
   "source": [
    "census_test = importing_censusTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(spam_dataset_train, spam_dataset_validate, spam_dataset_test)= import_processingSpam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree and Node Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following Decision Tree and Node classes are used to build and train a decision treen and train/ predict labels using this model. The tree is built by beginning with the whole data set of either Spam or Census. It then recursively builds the tree by calculating the minimum impurity/weighted entropy resulting from a chosen split for each depth. In order to determine if a node is a leaf node, three types of thresholds are used. The thresholds set in place are: maximum depth for the tree, composition of the current node, and the number of data points in the current node. The maximum depth was tinkered with to find that a depth of 12 worked best for Spam data, and a depth of 10 worked best for Census data. If a node was found to have 5 or less data points, it was deemed as a leaf node. This threshold was put in place to prevent over-fitting. Furthermore, each node was checked for percent composition of class 0 and class 1 data points. If the composition of one of the two classes was found to be less than a set threshold (either 0.001 or 0.0001), the node was deemed a leaf. If the subset of the original data at the current node was found to hit one of these thresholds, the node is a leaf. \n",
    "### The thresholds for categorical and continuous data were determined in two different ways. If a feature contained categorical variables, the threshold looked for the number of data points in the data that had feature values that were equal to the variable. (i.e. data[datapoint, feature]== variable). Although this doesn't take into account all possible combinations at each iteration, it still allows the data to be split over other variables in the feature deeper into the tree, essentially taking into account all the combinations of data. If a feature's variables were continuous, the variables were binned randomly for each split in the tree. The random binning was  chosen to reduce the number of splitting points/thresholds the continuous feature may have. This process was performed for each variable within each feature and each feature for the data. Data points that satisfy the threshold became the left node, and data points that don't satisfy the threshold became the right node.\n",
    "### The Node Class is used for storing the data gathered at each split. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "\n",
    "    def __init__(self, master_data, categorial_data, continuous_data, max_depth,\n",
    "                 min_impurity, features_allowed):\n",
    "        self.master_data = master_data\n",
    "        self.categorial_data = categorial_data\n",
    "        self.continuous_data = continuous_data\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity = min_impurity\n",
    "        self.allCombinations_categorical = None\n",
    "        self.rootNode = None\n",
    "        self.features_allowed = features_allowed\n",
    "\n",
    "    def impurity(self,left_label_hist, right_label_hist):\n",
    "        ''' Method calculates the impurity for the given dataset. Has two possible classes --> 0,1\n",
    "        Input is a dictionary of {key,value} --> {class,freq}'''\n",
    "        \n",
    "        total_left = sum(left_label_hist.values())\n",
    "        total_right = sum(right_label_hist.values())\n",
    "        \n",
    "        if (total_left + total_right) == 0:\n",
    "            return float(0) \n",
    "            \n",
    "        if total_left == 0:\n",
    "            left_entropy = 0\n",
    "        else:\n",
    "            prbL_0 = (left_label_hist[0])/float(total_left)\n",
    "            prbL_1 = (left_label_hist[1])/float(total_left)\n",
    "            left_entropy = -1*sum([prbL_0*np.log2(prbL_0), prbL_1*np.log2(prbL_1)])\n",
    "            if np.log2(prbL_0)== -float(\"inf\"):\n",
    "                left_entropy = float(0)\n",
    "            if np.log2(prbL_1)== -float(\"inf\"):\n",
    "                left_entropy = float(0)\n",
    "            \n",
    "        if total_right == 0:\n",
    "            right_entropy = 0\n",
    "        else:\n",
    "            prbR_0 = (right_label_hist[0])/float(total_right)\n",
    "            prbR_1 = (right_label_hist[1])/float(total_right)\n",
    "            right_entropy = -1*sum([prbR_0*np.log2(prbR_0), prbR_1*np.log2(prbR_1)])\n",
    "            if np.log2(prbR_0)== -float(\"inf\"):\n",
    "                right_entropy = float(0)\n",
    "            if np.log2(prbR_1)== -float(\"inf\"):\n",
    "                right_entropy = float(0)\n",
    "\n",
    "        impurity = ((total_left*left_entropy)+(total_right*right_entropy))/(total_left+total_right)\n",
    "        return impurity \n",
    "    \n",
    "    def composition(self,data):\n",
    "        '''This method will calculate the entropy for a given set of data. '''\n",
    "        master_data = self.master_data\n",
    "        num_0 = 0\n",
    "        num_1 = 0\n",
    "        for row_ind in data:\n",
    "            num = int(master_data[row_ind,master_data.shape[1]-1])\n",
    "            if num == 0:\n",
    "                num_0 = float(num_0 +1)\n",
    "            else:\n",
    "                num_1 = float(num_1+1)\n",
    "        \n",
    "        return min(num_0/(num_0+num_1),num_1/(num_0+num_1))\n",
    "    \n",
    "    def entropy(self,data):\n",
    "        ##calculate entropy for current parent node\n",
    "        master_data = self.master_data\n",
    "        num_0 = 0\n",
    "        num_1 = 0\n",
    "        for row_ind in data:\n",
    "            num = int(master_data[row_ind,master_data.shape[1]-1])\n",
    "            if num == 0:\n",
    "                num_0 = float(num_0 +1)\n",
    "            else:\n",
    "                num_1 = float(num_1+1)\n",
    "        total = num_0 + num_1\n",
    "        if num_0 == 0:\n",
    "            return -1*(num_1/total)*np.log2(num_1/total)\n",
    "        if num_1 == 0:\n",
    "            return -1*(num_0/total)*np.log2(num_0/total)\n",
    "        return -1*sum([(num_0/total)*np.log2(num_0/total), (num_1/total)*np.log2(num_1/total)])\n",
    "        \n",
    "        \n",
    "    def segementer(self,data):\n",
    "        '''Method will look through all possible splits, calcualte impurity, and return the combination\n",
    "        that results in min impurity.'''\n",
    "        \n",
    "        master_data = self.master_data\n",
    "        label = master_data[:,master_data.shape[1]-1]\n",
    "        categories = self.categorial_data\n",
    "        continuous = self.continuous_data\n",
    "        curr_impurity = math.pow(10,10)\n",
    "        toRtn_left = None\n",
    "        toRtn_right = None\n",
    "        min_split = None\n",
    "\n",
    "        if self.features_allowed < self.master_data.shape[1]-1: \n",
    "            feature_indices = np.random.choice(range(self.master_data.shape[1]-1), size = self.features_allowed, replace = True)\n",
    "        else:\n",
    "            feature_indices = range(self.master_data.shape[1]-1)\n",
    "\n",
    "\n",
    "        ##iterate through feature indices (column indices) to get the splits for each column\n",
    "        for col_ind in feature_indices:\n",
    "            if col_ind in categories:\n",
    "                categorical = True\n",
    "                poss_splits = np.unique(master_data[:,col_ind])\n",
    "            else:\n",
    "                categorical = False\n",
    "                if len(np.unique(master_data[:,col_ind])) >=4:\n",
    "                    poss_splits = np.sort(random.sample(np.unique(master_data[:,col_ind]), 4))\n",
    "                else:\n",
    "                    poss_splits = np.unique(master_data[:,col_ind])\n",
    "            for pos in poss_splits:\n",
    "                left_count = {0:0,1:0}\n",
    "                right_count = {0:0,1:0}\n",
    "                left_mat = []\n",
    "                right_mat = []\n",
    "                for row_ind in data:\n",
    "                    if categorical == True:\n",
    "                        if (master_data[row_ind,col_ind] == pos) and (label[row_ind]==0):\n",
    "                            left_count[0] = left_count[0]+1\n",
    "                            left_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] == pos) and (label[row_ind]==1):\n",
    "                            left_count[1]=left_count[1]+1\n",
    "                            left_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] != pos) and (label[row_ind]==0):\n",
    "                            right_count[0] = right_count[0]+1\n",
    "                            right_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] != pos) and (label[row_ind]==1):\n",
    "                            right_count[1]=right_count[1]+1\n",
    "                            right_mat.append(row_ind)\n",
    "                    else:\n",
    "                        \n",
    "                        if (master_data[row_ind,col_ind] <= pos) and (label[row_ind]==0):\n",
    "                            left_count[0] = left_count[0]+1\n",
    "                            left_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] <= pos) and (label[row_ind]==1):\n",
    "                            left_count[1]=left_count[1]+1\n",
    "                            left_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] > pos) and (label[row_ind]==0):\n",
    "                            right_count[0] = right_count[0]+1\n",
    "                            right_mat.append(row_ind)\n",
    "                        if (master_data[row_ind,col_ind] > pos) and (label[row_ind]==1):\n",
    "                            right_count[1]=right_count[1]+1\n",
    "                            right_mat.append(row_ind)\n",
    "                calc_impurity = self.impurity(left_count, right_count)\n",
    "                if (calc_impurity <= curr_impurity):\n",
    "                    curr_impurity = calc_impurity \n",
    "                    min_split = (col_ind, pos)\n",
    "                    toRtn_left = left_mat\n",
    "                    toRtn_right = right_mat\n",
    "        return (min_split, toRtn_left, toRtn_right)\n",
    "    \n",
    "    \n",
    "    def train(self,data,n):\n",
    "        '''This method recursively builds a decision tree and trains the split at each node.\n",
    "        Input - data matrix and vector of labels. Will increment depth here. \n",
    "        Nodes carry the following information: left, right, current split'''\n",
    "        #need to set up root node and check for entropy of current node AFTER each split -- if min has been reached, DON'T split\n",
    "        #OR if min-impurity is reached from creating a split -- stop\n",
    "        #data = range(self.master_data.shape[0])\n",
    "        self.rootNode = self.growTree(data,0,n)\n",
    "        return self\n",
    "\n",
    "    def growTree(self, data, depth,n):\n",
    "        \n",
    "        currImpurity = self.composition(data)\n",
    "\n",
    "        if (currImpurity <= self.min_impurity) or (depth >= self.max_depth) or (len(data)<= n):\n",
    "            currLabel = self.set_label(data)\n",
    "            return Node(left = None, right =None, split_rule = None, label =currLabel)            \n",
    "        else:\n",
    "            (curr_split, left_matrix, right_matrix) = self.segementer(data)\n",
    "            if len(right_matrix) == 0:\n",
    "                ## create leaf node with the majority value of the left matrix\n",
    "                currLabel = self.set_label(left_matrix)\n",
    "                return Node(left = None, right= None, split_rule = None, label = currLabel)\n",
    "            if len(left_matrix) == 0:\n",
    "                currLabel = self.set_label(right_matrix)\n",
    "                return Node(left = None, right= None, split_rule = None, label = currLabel)\n",
    "            else:\n",
    "                return Node(left = self.growTree(left_matrix, depth+1, n), \n",
    "                            right = self.growTree(right_matrix, depth+1, n), split_rule = curr_split,\n",
    "                            label= None)\n",
    "\n",
    "    def predict(self,data):\n",
    "        '''This method will predict the labels for new test data matrices. Data is inputted as a numpy matrix. '''\n",
    "        predicted_values = []\n",
    "        for row_ind in range(data.shape[0]):\n",
    "            currNode = self.rootNode\n",
    "            depth = 0\n",
    "            toRtn = ''\n",
    "            while (currNode.label == None):\n",
    "                (colNum, num_range) = currNode.split_rule #tupule (feature column number, subset list/integer)\n",
    "                toRtn = toRtn + str(colNum) + \" \" + str(num_range)\n",
    "                if colNum in self.categorial_data:\n",
    "                    if data[row_ind, colNum] == num_range:\n",
    "                        currNode = currNode.left\n",
    "                        toRtn = toRtn + \" \"+ 'left'\n",
    "                    else:\n",
    "                        currNode = currNode.right\n",
    "                        toRtn = toRtn + \" \"+ 'right'\n",
    "                else:\n",
    "                    if data[row_ind, colNum] <= num_range:\n",
    "                        currNode = currNode.left\n",
    "                        toRtn = toRtn + \" \"+ 'left'\n",
    "                    else:\n",
    "                        currNode = currNode.right\n",
    "                        toRtn = toRtn + \" \"+ 'right'\n",
    "                \n",
    "                if row_ind == 1:\n",
    "                    print toRtn\n",
    "                depth = depth + 1\n",
    "           \n",
    "            predicted_values.append(currNode.label)\n",
    "\n",
    "        return predicted_values\n",
    "\n",
    "    def set_label(self, data):\n",
    "        '''This method recognizes leaf nodes and returns the majority label within the node. \n",
    "        A node is considered a leaf node if the tree has reached  a maximum depth or if the tree has reached a\n",
    "        minimum impurity threshold. This shall be indicated by no split rule.'''\n",
    "        master_data = self.master_data\n",
    "        [num_0,num_1] = [0,0]\n",
    "        for row_ind in data:\n",
    "            num = int(master_data[row_ind,master_data.shape[1]-1])\n",
    "            if num == 0:\n",
    "                num_0 = num_0 +1\n",
    "            else:\n",
    "                num_1 = num_1+1\n",
    "        return [num_0,num_1].index(max([num_0,num_1]))\n",
    "\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self, left, right, split_rule, label):\n",
    "        self.left = left #will indicate binary \"yes\"\n",
    "        self.right = right #will indicate binary \"no\"\n",
    "        self.split_rule = split_rule\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predicted_values, trueLabels):\n",
    "    count = 0\n",
    "    for v in range(len(predicted_values)):\n",
    "        if predicted_values[v] == trueLabels[v]:\n",
    "            count = count +1 \n",
    "    return float(count)/len(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spam_validation_test():\n",
    "    spam_categorial_data = []\n",
    "    spam_continuous_data = range(spam_dataset_train.shape[1]-1) #last column of matrix is labels\n",
    "    tree = DecisionTree(spam_dataset_train,spam_categorial_data,\n",
    "                    spam_continuous_data,12, 0.0001, spam_dataset_train.shape[1]-1)\n",
    "    tree.train(range(spam_dataset_train.shape[0]), n=5)\n",
    "    spam_pred = tree.predict(spam_dataset_validate)\n",
    "    accuracy_validation = accuracy(spam_pred,spam_dataset_validate[:,spam_dataset_validate.shape[1]-1])\n",
    "    spam_pred2 = tree.predict(spam_dataset_test)\n",
    "    return accuracy_validation, spam_pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following tupule follow the splits that a radomly chosen data point encounters through the Spam decision tree. The first integer indicates the feature number/column number of the data matrix, the second integer indicates the variable of the given feature, 'left' indicates that the data point <= to the given feature&variable combination, 'right' indicates that the data point > the given feature&variable combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.0 left\n",
      "44 18.0 left\n",
      "19 0.0 left\n",
      "43 0.0 left\n",
      "0 0.0 left\n",
      "16 0.0 left\n",
      "45 7.0 left\n",
      "26 0.0 left\n",
      "46 0.0 left\n",
      "49 4.0 left\n",
      "48 2.0 left\n",
      "27 0.0 left\n",
      "39 0.0 right\n",
      "48 1.0 right\n",
      "5 0.0 left\n",
      "44 45.0 left\n",
      "43 0.0 left\n",
      "47 0.0 left\n",
      "3 0.0 left\n",
      "45 60.0 left\n"
     ]
    }
   ],
   "source": [
    "validation_accuracy3, predictions_spam = spam_validation_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple parameters were manually tried to find those that ran quickly and created an accurate descision tree. In the end, the following parameters were found to be best for census data:  max depth = 12, minimum impurity/minimum percent composition = 0.0001. he validation accuracy for spam data is as follows and was found to be higher with a binarized data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8442575406032483"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following tupule follow the splits that a radomly chosen data point encounters through the Census decision tree. The first integer indicates the feature number/column number of the data matrix, the second integer indicates the variable of the given feature, 'left' indicates that the data point <= or == to the given feature&variable combination, 'right' indicates that the data point > or != the given feature&variable combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def census_validation_test():\n",
    "    census_categorical_data = [1,3,5,6,7,8,9,13]\n",
    "    census_continuous_data = [0,2,4,10,11,12] \n",
    "    censusTree = DecisionTree(census_train, census_categorical_data, census_continuous_data, \n",
    "                              10,0.001,census_train.shape[1]-1)\n",
    "    censusTree.train(range(census_train.shape[0]), n=0)\n",
    "    census_pred = censusTree.predict(census_validate)\n",
    "    validation_acc = accuracy(census_pred,census_validate[:,census_validate.shape[1]-1])\n",
    "    census_pred2 = censusTree.predict(census_test)\n",
    "    #print census_pred2\n",
    "    return validation_acc, census_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.0 left\n",
      "10 4386.0 left\n",
      "4 7.0 right\n",
      "3 11.0 left\n",
      "11 1741.0 left\n",
      "0 33.0 right\n",
      "6 4.0 right\n",
      "0 65.0 left\n",
      "12 25.0 right\n",
      "0 35.0 right\n",
      "5 2.0 right\n",
      "10 4865.0 left\n",
      "4 12.0 right\n",
      "5 4.0 right\n",
      "11 2205.0 left\n",
      "12 48.0 left\n",
      "6 4.0 right\n",
      "9 1.0 right\n",
      "6 1.0 right\n",
      "0 34.0 right\n"
     ]
    }
   ],
   "source": [
    "validation_acc, census_predictions = census_validation_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple parameters were manually tried to find those that ran quickly and created an accurate descision tree. In the end, the following parameters were found to be best for census data:  max depth = 10, minimum impurity/minimum percent composition = 0.001. The validation accuracy for census is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "### The following class is used to build a Random Forest. The random forest algorithm generates multiple descision trees trained on the test data for Spam/Census, validated on respective validation sets, and tested on their respective test sets. No new methods were built for Random Forest. The parameters for Random Forest are the same as for Descision Tree. Each tree in the forest is trained on the given test data, validated on the validation data, and tested/predicts on the test data. \n",
    "### In retrospect, bagging or ADAboost would have improved the accuracy of the random forest drastically, as each tree that is built would be weighted to a certain degree depending on its error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    ''' The following class is a subclass of Decision Tree. It uses the methods described in Decision tree to \n",
    "    perform bagging and implements Random Forests. The sampling for attribute bagging is done with sqrt(d) features.'''\n",
    "    \n",
    "    def __init__(self,master_data, categorial_data, continuous_data, max_depth, min_impurity,\n",
    "                 max_sampling, feature_num, dataset_validate):\n",
    "        self.master_data = master_data\n",
    "        self.categorial_data = categorial_data\n",
    "        self.continuous_data = continuous_data\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_sampling = max_sampling\n",
    "        self.feature_num = feature_num\n",
    "        self.predicted_all = None\n",
    "        self.dataset_validate = dataset_validate\n",
    "    \n",
    "    def bagging(self, data_pred, num_samples,min_num):\n",
    "        master_data = self.master_data\n",
    "        max_sampling = self.max_sampling\n",
    "        predicted_all = np.array([])\n",
    "        rootNode_all = {}\n",
    "        i = 0\n",
    "        while i <= max_sampling:\n",
    "            data_forest = np.random.choice(range(master_data.shape[0]), size = num_samples, replace= True)\n",
    "            predicted, rootNode = self.bagging_helper(data_pred,data_forest,min_num)\n",
    "            \n",
    "            if len(predicted_all) == 0:\n",
    "                predicted_all = predicted\n",
    "            else:\n",
    "                predicted_all = np.vstack((predicted_all,predicted))\n",
    "                \n",
    "            if rootNode not in rootNode_all.keys():\n",
    "                rootNode_all[rootNode]=1\n",
    "            else:\n",
    "                rootNode_all[rootNode]=rootNode_all[rootNode]+1\n",
    "            i = i + 1\n",
    "            \n",
    "        self.predicted_all = predicted_all\n",
    "        toRtn = np.apply_along_axis(self.averaging,0,predicted_all)\n",
    "        return toRtn, rootNode_all\n",
    "            \n",
    "    def bagging_helper(self, data_pred, data_forest,min_num):\n",
    "        '''This method will create Decision tree objects to run using the sampled data from bagging method.'''  \n",
    "        tree = DecisionTree(self.master_data,self.categorial_data,\n",
    "                    self.continuous_data,self.max_depth, self.min_impurity, self.feature_num)\n",
    "        tree.train(data_forest, min_num)\n",
    "        currRoot = tree.rootNode.split_rule\n",
    "        predicted = tree.predict(data_pred) #label for each sample\n",
    "        \n",
    "        return predicted, currRoot\n",
    "    \n",
    "    def averaging(self,predicted):\n",
    "        curr0 = np.where(predicted == 0)[0].size\n",
    "        curr1 = np.where(predicted==1)[0].size\n",
    "        return [curr0,curr1].index(max([curr0,curr1]))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_census():\n",
    "    census_categorical_data = [1,3,5,6,7,8,9,13]\n",
    "    census_continuous_data = [0,2,4,10,11,12] \n",
    "    forest = RandomForest(master_data = census_train, categorial_data = census_categorical_data,\n",
    "                    continuous_data = census_continuous_data, max_depth = 10, min_impurity= 0.001, \n",
    "                    max_sampling = 30, feature_num = int(math.sqrt(census_train.shape[1])),\n",
    "                     dataset_validate = census_validate)\n",
    "    bagged, rootNodes = forest.bagging(census_validate, census_train.shape[0], 0)\n",
    "    acc = accuracy(bagged, census_validate[:,census_validate.shape[1]-1])\n",
    "    bagged_test = forest.bagging(census_test, census_test.shape[0], 0)\n",
    "    return (bagged_test,acc, rootNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagged_census, acc_census, rootNodes = forest_census()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following returns the ((feature, variable), frequency of split seen as root split in spam descision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((5, 2.0), 8), ((7, 0.0), 5), ((3, 9.0), 4)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "sorted_roots_census = sorted(rootNodes.items(), key=operator.itemgetter(1), reverse= True)[0:3]\n",
    "print sorted_roots_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forest_spam():\n",
    "    spam_categorial_data = []\n",
    "    spam_continuous_data = range(spam_dataset_train.shape[1]-1) \n",
    "    forest = RandomForest(master_data = spam_dataset_train, categorial_data = spam_categorial_data,\n",
    "                    continuous_data = spam_continuous_data, max_depth = 12, min_impurity= 0.0001, \n",
    "                    max_sampling = 30, feature_num = int(math.sqrt(spam_dataset_train.shape[1])),\n",
    "                     dataset_validate = spam_dataset_validate)\n",
    "    bagged, rootNodes = forest.bagging(spam_dataset_validate, spam_dataset_train.shape[0], 0)\n",
    "    acc = accuracy(bagged,spam_dataset_validate[:,spam_dataset_validate.shape[1]-1])\n",
    "    ##running trained forest on test set\n",
    "    bagged_test = forest.bagging(spam_dataset_test, spam_dataset_test.shape[0],0)\n",
    "    return (bagged_test, acc, rootNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagged_spam, acc_spam, rootNodes_spam = forest_spam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following returns the ((feature, variable), frequency of split seen as root split in spam descision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((10, 0.0), 3), ((43, 0.0), 3), ((5, 0.0), 2)]\n"
     ]
    }
   ],
   "source": [
    "sorted_roots_spam = sorted(rootNodes_spam.items(), key=operator.itemgetter(1), reverse = True)[0:3]\n",
    "print sorted_roots_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "### The kaggle score for a random forest on census data is: 0.762\n",
    "### The kaggle score for a random forest on spam data is: 0.711\n",
    "### The scores for random forest and a singular descision tree did not differ much. Furthermore, the validation scores for both datasets using random forest or a singular descision are both approximately 0.85. This indicates that the tree has over-fit the training data and the random forest can be made more powerful through boosting, ADAboost, cross-validation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToCSV(fileName, predictions):\n",
    "    import csv\n",
    "    with open(fileName, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Id','Category'])\n",
    "        shl = 1\n",
    "        for pred_l in predictions:\n",
    "            writer.writerow([shl,int(pred_l)])\n",
    "            shl = shl + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writeToCSV('census_nonforest.csv',census_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToCSV('spam_nonforest.csv',predictions_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writeToCSV('census_test_forest.csv',bagged_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writeToCSV('spam_test_forest2.csv',bagged_spam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
